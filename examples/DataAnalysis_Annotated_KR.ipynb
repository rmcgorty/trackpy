{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Data Analysis!\n",
    "Here you will pull out the main details of the particles you have tracked. \n",
    "There are three main sections:\n",
    "* Diffusion \n",
    "* Conformation\n",
    "* Ergodicity\n",
    "***\n",
    "To jump to the Diffusion functions click [here](#Diffusion-Methods).\n",
    "\n",
    "To jump to the Conformational functions click [here](#Conformational-Methods)\n",
    "\n",
    "To jump to Ergodicity functions click [here](#Ergodicity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns               #version 0.9.0\n",
    "import warnings\n",
    "import pims                         #version 0.4.1\n",
    "import trackpy as tp                #version 0.4.1\n",
    "import pandas as pd                 #version 0.23.4\n",
    "import os\n",
    "import csv                          #version 1.0\n",
    "import numpy as np                  #version 1.15.1\n",
    "import imageio                      #version 2.4.1\n",
    "import pickle                       #revision: 72223\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import cv2                          #version 3.4.3\n",
    "import sys\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$Revision: 72223 $\n"
     ]
    }
   ],
   "source": [
    "print pickle.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadIndividualDataset(f_root, subdir_name):\n",
    "    ''' Loads an individual dataset\n",
    "    \n",
    "    Input:\n",
    "        f_root = root directory for data\n",
    "        subdir_name = name of the subdirectory containing data\n",
    "        frames (optional) = option to grab frames data\n",
    "        \n",
    "    Output:\n",
    "        centers = dictionary containing data for the individual dataset\n",
    "    '''\n",
    "    sub_dir = os.path.join(f_root, subdir_name)\n",
    "    \n",
    "    centers_loc = os.path.join(sub_dir, 'centers.p')\n",
    "    centers = pickle.load(open(centers_loc, \"rb\" ))\n",
    "\n",
    "    return centers\n",
    "\n",
    "def loadMultipleDatasets(f_root, subdir_names):  \n",
    "    ''' Loads multiple datasets into dict\n",
    "    \n",
    "    Input:\n",
    "        f_root = root directory for data\n",
    "        subdir_names = list of subdirectories to pull data for\n",
    "        frames (optional) = option to grab frames data\n",
    "    '''\n",
    "    return {subdir_name: loadIndividualDataset(f_root, subdir_name) \\\n",
    "            for subdir_name in subdir_names}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Methods\n",
    "* Get differences between frames\n",
    "* Plot histogram of differences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDiffs(datasets):\n",
    "    ''' Takes dictionary (output of `loadMultipleDatasets`) and iterates through each particle.\n",
    "    For each particle, sweeps through and finds all diffs between frames (for each possible window\n",
    "    size) and adds to master dict (one for each axis)\n",
    "    \n",
    "    Input:\n",
    "        datasets = dictionary of datasets, output of `loadMultipleDatasets`\n",
    "        \n",
    "    Output:\n",
    "        dictionary of diffs for each window size (one for each axis)\n",
    "    '''\n",
    "    diffs_x = defaultdict(list)\n",
    "    diffs_y = defaultdict(list)\n",
    "    diffs_comb = defaultdict(list)\n",
    "    \n",
    "    # iterate through datasets\n",
    "    for dataset, data in datasets.iteritems():\n",
    "        # iterate through videos\n",
    "        for video, particles in data['all_centers_dict_normed'].items():\n",
    "            # iterate through particles\n",
    "            for particle, center_data in particles.items():\n",
    "                x_centers = center_data['x']\n",
    "                y_centers = center_data['y']\n",
    "                \n",
    "                # iterate through possible sliding window sizes\n",
    "                for window in range(1, len(x_centers)):\n",
    "                    for ind_start in range(len(x_centers)-window):\n",
    "                        diffs_x[window].append(x_centers[window+ind_start] - x_centers[ind_start]) \n",
    "                        diffs_y[window].append(y_centers[window+ind_start] - y_centers[ind_start])\n",
    "                        diffs_comb[window].append(diffs_x[window][-1])\n",
    "                        diffs_comb[window].append(diffs_y[window][-1])\n",
    "\n",
    "    return {'x': diffs_x,\n",
    "           'y': diffs_y,\n",
    "           'comb': diffs_comb}\n",
    "\n",
    "\"\"\" Command to plot \"\"\"\n",
    "def plotIndividualHistOfDiffs(diff_dict, window, binsize, axis):\n",
    "    ''' Plots histogram of diffs for particular window size, axis\n",
    "    \n",
    "    Input:\n",
    "        diff_dict = axis dict of differences\n",
    "        window = size of the window to plot hist (number of frames)\n",
    "        axis = string of axis (for labeling)\n",
    "        \n",
    "    Output:\n",
    "        None, plots individual histogram\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.hist(diff_dict[window], bins = binsize);\n",
    "    plt.title('%s-diffs: %d frames' % (axis, window), fontsize=15)\n",
    "    plt.xlabel('pixels', fontsize=15)\n",
    "    plt.ylabel('count', fontsize=15)\n",
    "    plt.legend(fontsize=15)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(diff_dict[window], bins = binsize);\n",
    "    plt.title('%s-diffs: %d frames' % (axis, window), fontsize=15)\n",
    "    plt.xlabel('pixels', fontsize=15)\n",
    "    plt.ylabel('count', fontsize=15)\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    \n",
    "def plotAllHistOfDiffs(diffs, window,binsize):\n",
    "    ''' Calls `plotIndividualHistOfDiffs` for each axis (x, y, combined) for a \n",
    "    particular window\n",
    "    \n",
    "    Input:\n",
    "        diffs = dictionary of diffs (one for each axis)\n",
    "        window = size of the window to plot hists (number of frames)\n",
    "        \n",
    "    Output:\n",
    "        None, plots individual histograms\n",
    "    '''\n",
    "    for axis, axis_dict in diffs.items():\n",
    "        plotIndividualHistOfDiffs(axis_dict, window, binsize, axis)\n",
    "def mean(vals_list):\n",
    "    ''' Returns mean of a list of values. Because this doesn't exist\n",
    "    natively in python!\n",
    "    \n",
    "    Input:\n",
    "        vals_list = list of values\n",
    "        \n",
    "    Output:\n",
    "        single value (float), mean of the list\n",
    "    '''\n",
    "    return sum(vals_list)/float(len(vals_list))\n",
    "\n",
    "def calculateMSDs(diffs_dict):\n",
    "    ''' Calculates the msd values for each time-frame, for each axis\n",
    "    \n",
    "    Input:\n",
    "        diffs_dict = dictionary of diffs (one for each axis), output of `getDiffs`\n",
    "        \n",
    "    Output:\n",
    "        diffs_squared_dict = dictionary of MSD values {frame: pixels^2} for each axis\n",
    "    '''\n",
    "    \n",
    "    diffs_squared_dict = {'x': {0: 0}, 'y': {0: 0}, 'comb': {0: 0}}\n",
    "    for axis, axis_dict in diffs_dict.iteritems():\n",
    "        for window, diffs in axis_dict.iteritems():\n",
    "            diffs_squared_dict[axis][window] = mean([diff**2 for diff in diffs])\n",
    "        \n",
    "    return diffs_squared_dict\n",
    "\n",
    "def plotIndividualMsd(msd_dict, axis, line_type='-o'):\n",
    "    ''' Plots MSD for individual axis\n",
    "    \n",
    "    Input:\n",
    "        msd_dict = MSD values all axes (output of `calculateMSDs`)\n",
    "        axis = string of the axis to plot\n",
    "        line_type (optional) = line type for the plot\n",
    "        \n",
    "    Output:\n",
    "        None, plots the individual MSD\n",
    "    '''\n",
    "    \n",
    "    plt.figure();\n",
    "    \n",
    "    vals = sorted([(k, v) for k, v in msd_dict.items()])\n",
    "    x = [v[0] for v in vals]\n",
    "    y = [v[1] for v in vals]\n",
    "    \n",
    "    plt.plot(x, y, line_type)\n",
    "    plt.title('MSD for axis: %s' % axis, fontsize=15)\n",
    "    plt.xlabel('frames', fontsize=15)\n",
    "    plt.ylabel('pixels^2', fontsize=15)\n",
    "    \n",
    "def plotAllMsds(msd_dict, line_type='-o'):\n",
    "    for axis, axis_dict in msd_dict.items():\n",
    "        plotIndividualMsd(axis_dict, axis, line_type=line_type)\n",
    "        \n",
    "def linearFitIndividualMsd(axis_dict, axis, start_frame=0, end_frame=-1, plot=True, line_type='-o'):\n",
    "    if end_frame == -1:\n",
    "        end_frame = len(axis_dict.values()) - 1\n",
    "        \n",
    "    # ensure proper sorting of x, y\n",
    "    vals = sorted([(k, v) for k, v in axis_dict.items()])\n",
    "    x = [v[0] for v in vals][start_frame:end_frame]\n",
    "    y = [v[1] for v in vals][start_frame:end_frame]\n",
    "    \n",
    "    linear_fits = np.polyfit(x,y,1)\n",
    "    linear_fits_fn = np.poly1d(linear_fits) \n",
    "\n",
    "    if plot:\n",
    "        plotIndividualMsd(axis_dict, axis, line_type=line_type)\n",
    "        plt.plot(x, linear_fits_fn(x), '-');\n",
    "        \n",
    "    return linear_fits\n",
    "\n",
    "def linearFitAllAxes(msd_dict, start_frame=0, end_frame=-1, plot=True, line_type='-o', linewidth=2):\n",
    "    linear_fits = {}\n",
    "    for axis, axis_dict in msd_dict.items():\n",
    "        linear_fits[axis] = linearFitIndividualMsd(\\\n",
    "            axis_dict, axis, start_frame=start_frame, end_frame=end_frame, plot=plot, line_type=line_type)\n",
    "        \n",
    "    return linear_fits\n",
    "\n",
    "def logLogMsdData(msd_dict):\n",
    "    msd_dict_ll = {}\n",
    "    for axis, axis_dict in msd_dict.items():\n",
    "        msd_dict_ll[axis] = {}\n",
    "        for window, msd_val in axis_dict.items():\n",
    "            if window == 0:\n",
    "                continue\n",
    "            \n",
    "            msd_dict_ll[axis][math.log(window)] = math.log(msd_val)\n",
    "    \n",
    "    return msd_dict_ll\n",
    "\n",
    "def calculateIndividualMsdStats(axis_dict, axis_dict_ll, start_frame=0, end_frame=-1):\n",
    "    stats = {'start_frame': start_frame, 'end_frame': end_frame}\n",
    "    stats['linear_fit'] = linearFitIndividualMsd(\\\n",
    "            axis_dict, '', start_frame=start_frame, end_frame=end_frame, plot=False)\n",
    "    \n",
    "    stats['linear_fit_ll'] = linearFitIndividualMsd(\\\n",
    "            axis_dict_ll, '', start_frame=start_frame, end_frame=end_frame, plot=False)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def calculateAllMsdStats(msd_dict, start_frame=0, end_frame=-1):\n",
    "    msd_dict_ll = logLogMsdData(msd_dict)\n",
    "\n",
    "    all_stats = {}\n",
    "    for axis, axis_dict in msd_dict.iteritems():\n",
    "        all_stats[axis] = calculateIndividualMsdStats(\\\n",
    "            axis_dict, msd_dict_ll[axis], start_frame=start_frame, end_frame=end_frame)\n",
    "        \n",
    "    return all_stats\n",
    "\n",
    "def loadDatasetsAndCalculateMsdStats(f_root, subdir_names, start_frame=0, end_frame=-1):\n",
    "    datasets = loadMultipleDatasets(f_root, subdir_names)\n",
    "    diffs_dict = getDiffs(datasets)\n",
    "\n",
    "    msd_dict = calculateMSDs(diffs_dict)\n",
    "    msd_dict_ll = logLogMsdData(msd_dict)\n",
    "\n",
    "    all_stats = calculateAllMsdStats(msd_dict, start_frame=start_frame, end_frame=end_frame)\n",
    "    return all_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Analysis: Load and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Conformation = \"Actin\"\n",
    "Condition = \"Ring\"\n",
    "date=\"20181205\"\n",
    "mass_used1='2500'\n",
    "mass_used2='3000'\n",
    "\n",
    "\n",
    "#Cloud_Location = \"\\\\\\ANDERSONLAB\\\\AndersonLab\"\n",
    "\n",
    "'''Enter the names of your files below, normally in day_Condition_Minmass format'''\n",
    "#group=\"10\"\n",
    "\n",
    "\n",
    "subdir_names=([\"JG_test_1_\"\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f_root = Cloud_Location+\"\\\\\"+User+\"\\\\DATA\\\\DNA_in_XL_systems\\\\Lightsheet\\\\\"+Conformation+\"\\\\\"+Condition+\"\\\\Data\\\\\"\n",
    "f_root=\"F:\\\\19_12_3_XLMicrotubules\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\19_12_3_XLMicrotubules\\\n",
      "['JG_test_1_']\n"
     ]
    }
   ],
   "source": [
    "print f_root\n",
    "print subdir_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Analysis: Go through sequentially\n",
    "### **Start with calculating all MSD data below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = loadMultipleDatasets(f_root, subdir_names)\n",
    "diffs_dict = getDiffs(datasets)\n",
    "#binsize=45\n",
    "\n",
    "msd_dict = calculateMSDs(diffs_dict)\n",
    "#msd_dict_ll = logLogMsdData(msd_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To jump to the MSD plotting (linear-linear) click [here](#2.-Plotting-MSDs:-linear-and-log-log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-5dae3fd54b97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets['Entangled_video1_75000']['all_centers_dict_normed']['_1_MMStack_Pos0.ome.tif'][413L]['x'][140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cut = datasets['Entangled_video1_75000']['all_centers_dict_normed']['_1_MMStack_Pos0.ome.tif']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MSD: Histogram of displacements\n",
    "You can change the lagtime in frames below. \n",
    "\n",
    "**Want to save all of the differences?**\n",
    "You can draw out the values for whatever frame lagtime you want. If running multiple subdirectories together, save them under \"All\" condition or make a new folder with their combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lagtime_frames=1\n",
    "plotAllHistOfDiffs(diffs_dict,lagtime_frames,binsize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lagtime_frames=25\n",
    "plotAllHistOfDiffs(diffs_dict,lagtime_frames,binsize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lagtime_value = [1, 2, 3, 5, 10, 15, 20, 25, 30, 40]\n",
    "lagtime = [\"1\", \"2\", \"3\", \"5\", \"10\", \"15\", \"20\", \"25\", \"30\", \"40\"]\n",
    "Axis='comb'\n",
    "#Conformation=\"Ring\"\n",
    "#Condition = 'Actin'\n",
    "#Date = 'All'\n",
    "#mass_used=\"3000\"\n",
    "#group=\"All\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in (range(len(lagtime_value))):\n",
    "    values=diffs_dict[Axis][lagtime_value[i]] #will give displacement values for each particle with lagtime_value frames diff\n",
    "    w=open(f_root+\"displacements_\"+lagtime[i]+\"frames\".csv\",'wb')\n",
    "    writeFile=csv.writer(w)\n",
    "\n",
    "    for n in values:\n",
    "        writeFile.writerow([n])\n",
    "    w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265641"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diffs_dict['x'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. Plotting MSDs: linear and log-log\n",
    " *Note* If you need to acccess further info within the MSD dictionary, averaged out MSD data is stored within msd_dict as a dictionary of dictionaries. The key-value pair is the axis *(x, y, comb)* paired with the dictionary of values for that axis. \n",
    "**I.E.** msd_dict['y'] gives you all the frame and pixel values of the averaged out MSD dictionary in the *y* frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotAllMsds(msd_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Want to save the MSD values? Hint: yes you do. \n",
    "There are a couple different templates down there for where you can save .csv files to. You can also always just rewrite your own as well.\n",
    "\n",
    "There are two saving options: one will save one day's worth of data to **that day** but the other will save **several days'** worth of data to **Condition_ALL** folder (i.e., when running all the data you have for one condition, you want to save it all together)\n",
    "\n",
    "Recommended to save linear scale values and plot on log-log using Origin.\n",
    "\n",
    "Axis can be 'x', 'y' or 'comb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "axis='comb'\n",
    "#day='050919' #if running only 1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values=msd_dict[axis].values()\n",
    "\n",
    "w=open(f_root+Conformation+\"_\"+mass_used+\"\\\\\"+Conformation+\"_\"+date+\"_msd_vals_\"+axis+\".csv\",'wb')\n",
    "#w=open(\"\\\\\\ANDERSONLAB\\\\AndersonLab\\\\\"+User+\"\\\\DATA\\\\DNA_in_XL_systems\\\\Lightsheet\\\\\"+Conformation+\"\\\\\"+Condition+\"\\\\Data\\\\\"+Condition+\"_ALL\\\\msd_vals_\"+axis+\".csv\",'wb')\n",
    "writeFile=csv.writer(w)\n",
    "\n",
    "for n in values:\n",
    "    writeFile.writerow([n])\n",
    "w.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotAllMsds(msd_dict_ll, line_type='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option to save Log-log values (not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "axis='comb'\n",
    "day='042419' #if running only 1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values=msd_dict_ll[axis].values()\n",
    "\n",
    "w=open(\"\\\\\\ANDERSONLAB\\\\AndersonLab\\\\Kathryn_R\\\\DATA\\\\DNA_in_XL_systems\\\\Lightsheet\\\\\"+Conformation+\"\\\\\"+Condition+\"\\\\Data\\\\\"+Date+\"_\"+Condition+\"_\"+MassUsed+\"\\\\LOG_msd_vals_\"+axis+\".csv\",'wb')\n",
    "#w=open(\"\\\\\\ANDERSONLAB\\\\AndersonLab\\\\\"+User+\"\\\\DATA\\\\DNA_in_XL_systems\\\\Lightsheet\\\\\"+Conformation+\"\\\\\"+Condition+\"\\\\Data\\\\\"+Condition+\"_ALL\\\\LOG_msd_vals_\"+axis+\".csv\",'wb')\n",
    "writeFile=csv.writer(w)\n",
    "\n",
    "for n in values:\n",
    "    writeFile.writerow([n])\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fitting MSDs: linear and log-log\n",
    "\n",
    "You can tailor what frame range the fit is for, i.e. 0-24 is the first 25 frames\n",
    "The fit coordinates that pop up are the coordinates for a line, *i.e.* **m,b** in **y=mx+b**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_frame = 2\n",
    "end_frame = 25\n",
    "linear_fits = linearFitAllAxes(msd_dict, start_frame=start_frame, end_frame=end_frame, plot=True)\n",
    "print linear_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_frame = 2\n",
    "end_frame = 25\n",
    "fits = linearFitAllAxes(msd_dict_ll, start_frame=start_frame, end_frame=end_frame, line_type='-o')\n",
    "print(fits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformational Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic conformational functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateConformationsForDatasets(datasets, f_root, subdir, plot=True):\n",
    "    all_r_max_min = {}\n",
    "    r_max=[] #1.22.19\n",
    "    r_min=[] #1.22.19\n",
    "    for subdir_name, dataset in datasets.items():\n",
    "        print \"Calculating conformations for: %s\" % subdir_name\n",
    "        \n",
    "        # get list of videos to pull\n",
    "        centered_frames_loc = os.path.join(f_root, subdir_name, 'centered_frames')\n",
    "        print subdir_name\n",
    "        print centered_frames_loc\n",
    "        videos =  [video for video in os.listdir(centered_frames_loc) if video.endswith('.p')]\n",
    "        print videos\n",
    "        for video in videos:\n",
    "            data = pickle.load(open(os.path.join(centered_frames_loc, video), \"rb\" ))\n",
    "            all_r_max_min[video] = {}\n",
    "            for particle, img_stack in data.items():\n",
    "                r_max_min = calculateRMaxMin(img_stack, f_root, subdir_name, video, particle, plot=plot)\n",
    "                all_r_max_min[video][particle] = r_max_min                \n",
    "                \n",
    "\n",
    "        r_max_min_loc = os.path.join(f_root, subdir_name, 'r_max_min.p')\n",
    "        pickle.dump(all_r_max_min, open(r_max_min_loc, 'wb'))\n",
    "        print \"\\tFinished dataset!\"\n",
    "        \n",
    "def loadMultipleConformationalDatasets(f_root, subdir_names):\n",
    "    all_r_max_min = {}\n",
    "    for subdir_name in subdir_names:\n",
    "        r_max_min_loc = os.path.join(f_root, subdir_name, 'r_max_min.p')\n",
    "        print r_max_min_loc\n",
    "        all_r_max_min[subdir_name] = pickle.load(open(r_max_min_loc, \"rb\" ))\n",
    "    return all_r_max_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for calculating R_max, R_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadIndividualDataset(f_root, subdir_name):\n",
    "    ''' Loads an individual dataset\n",
    "    \n",
    "    Input:\n",
    "        f_root = root directory for data\n",
    "        subdir_name = name of the subdirectory containing data\n",
    "        frames (optional) = option to grab frames data\n",
    "        \n",
    "    Output:\n",
    "        centers = dictionary containing data for the individual dataset\n",
    "    '''\n",
    "    sub_dir = os.path.join(f_root, subdir_name)\n",
    "    \n",
    "    centers_loc = os.path.join(sub_dir, 'centers.p')\n",
    "    centers = pickle.load(open(centers_loc, \"rb\" ))\n",
    "\n",
    "    return centers\n",
    "\n",
    "def loadMultipleDatasets(f_root, subdir_names):  \n",
    "    ''' Loads multiple datasets into dict\n",
    "    \n",
    "    Input:\n",
    "        f_root = root directory for data\n",
    "        subdir_names = list of subdirectories to pull data for\n",
    "        frames (optional) = option to grab frames data\n",
    "    '''\n",
    "    return {subdir_name: loadIndividualDataset(f_root, subdir_name) \\\n",
    "            for subdir_name in subdir_names}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Rmax, Rmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateImageMean(image):\n",
    "    vals = []\n",
    "    for row in image:\n",
    "        vals.extend(row)\n",
    "\n",
    "    return mean(vals)\n",
    "\n",
    "def calculateImageThreshold(bkgd_mean):\n",
    "    ''' Calculate threshold for cutting off background. Very simple to \n",
    "    start, just greater than image mean \n",
    "    \n",
    "    '''\n",
    "    return bkgd_mean + 30\n",
    "    #return (255+bkgd_mean)/2.0\n",
    "\n",
    "def getLargestContour(contours):\n",
    "    # limit to largest contour (particle)\n",
    "    areas = []\n",
    "    for contour in contours:\n",
    "        ar = cv2.contourArea(contour)\n",
    "        areas.append(ar)\n",
    "    \n",
    "    max_area = max(areas)\n",
    "    max_area_index = areas.index(max_area)\n",
    "    return contours[max_area_index]\n",
    "\n",
    "def euclideanDistance(point_1, point_2):\n",
    "    return ((point_1[0] - point_2[0])**2. + (point_1[1] - point_2[1])**2.)**0.5\n",
    "\n",
    "def calculateRMaxMin(img_stack, f_root, subdir_name, video, particle, plot=True):\n",
    "    r_max_min = {}\n",
    "    \n",
    "    for frame, img in enumerate(img_stack):\n",
    "        bkgd_mean = calculateImageMean(img)\n",
    "        cutoff_val = calculateImageThreshold(bkgd_mean)\n",
    "        ret, thresh1 = cv2.threshold(img,cutoff_val,255,cv2.THRESH_BINARY)\n",
    "\n",
    "        # find object outlines\n",
    "        _, contours, hierarchy = cv2.findContours(thresh1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) \n",
    "        #print contours\n",
    "\n",
    "        # limit to biggest contour\n",
    "        primary_contour = getLargestContour(contours)\n",
    "\n",
    "        # get rectangle to approximate the contour\n",
    "        rect = cv2.minAreaRect(primary_contour)\n",
    "        box = cv2.boxPoints(rect)\n",
    "\n",
    "        # get midpoints from box vertices\n",
    "        anchor = box[0]\n",
    "        dists = []\n",
    "        for i, vertex in enumerate(box[1:]):\n",
    "            dists.append((euclideanDistance(anchor, vertex), i+1))\n",
    "\n",
    "        dists = sorted(dists)\n",
    "        len_min_axis = dists[0][0]\n",
    "        len_maj_axis = dists[1][0]\n",
    "\n",
    "        minor_vertex = box[dists[0][1]]\n",
    "        major_vertex = box[dists[1][1]]\n",
    "        anchor_2 = box[dists[2][1]]\n",
    "\n",
    "        delta_minor = ((anchor[0] - minor_vertex[0])/2., (anchor[1] - minor_vertex[1])/2.)\n",
    "        delta_major = ((anchor[0] - major_vertex[0])/2., (anchor[1] - major_vertex[1])/2.)\n",
    "\n",
    "        minor_mid_1 = anchor[0]-delta_minor[0], anchor[1]-delta_minor[1]\n",
    "        minor_mid_2 = anchor_2[0]+delta_minor[0], anchor_2[1]+delta_minor[1]\n",
    "\n",
    "        major_mid_1 = anchor[0]-delta_major[0], anchor[1]-delta_major[1]\n",
    "        major_mid_2 = anchor_2[0]+delta_major[0], anchor_2[1]+delta_major[1]\n",
    "\n",
    "        # add each frame to the dict\n",
    "        r_max_min[frame] = {'major': (major_mid_1, major_mid_2),\n",
    "                           'minor': (minor_mid_1, minor_mid_2),\n",
    "                           'r_max': len_maj_axis,\n",
    "                           'r_min': len_min_axis,}\n",
    "\n",
    "        # draw each image with Rmax/Rmin superimposed\n",
    "        if plot:\n",
    "            plt.imshow(img);\n",
    "            plt.grid(False);\n",
    "            plt.plot((minor_mid_1[0], minor_mid_2[0]), (minor_mid_1[1], minor_mid_2[1]), 'k-', color='white');\n",
    "            plt.plot((major_mid_1[0], major_mid_2[0]), (major_mid_1[1], major_mid_2[1]), 'k-', color='white');\n",
    "            loc_to_write = os.path.join(f_root, subdir_name, 'r_max_min/%s_%d_%d.png' % (video, particle, frame))\n",
    "            plt.savefig(loc_to_write);\n",
    "            plt.clf();\n",
    "        \n",
    "    return r_max_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for calculating Fluctuation Length L(t) = <|Rmax(t+tau) - Rmax(t)|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Methods for plotting L(t) = <abs(Rmax(t+tau) - Rmax(t))>\n",
    "'''\n",
    "\n",
    "def getRmaxDiffs(all_r_max_min):\n",
    "    ''' Takes dictionary (output of `loadMultipleConformationalDatasets`) and iterates through each particle.\n",
    "    For each particle, sweeps through and finds all diffs between frames for r_max (for each possible window\n",
    "    size) and adds to master dict (one for each axis)\n",
    "    \n",
    "    Input:\n",
    "        all_r_max_min = dictionary of conformational datasets, output of `loadMultipleConformationalDatasets`\n",
    "        \n",
    "    Output:\n",
    "    \n",
    "        dictionary of r_max diffs for each window size\n",
    "    '''\n",
    "    diffs_r_max = defaultdict(list)\n",
    "    r_max_save = {}\n",
    "    # iterate through datasets\n",
    "    for dataset, data in all_r_max_min.iteritems():\n",
    "        # iterate through videos\n",
    "        for video, particles in data.items():\n",
    "            # iterate through particles\n",
    "            for particle, conform_data in particles.items():\n",
    "                r_max_vals = [conform_data[frame]['r_max'] for frame in sorted(conform_data.keys())]\n",
    "                  \n",
    "\n",
    "                # iterate through possible sliding window sizes\n",
    "                for window in range(0, len(r_max_vals)):\n",
    "                    for ind_start in range(len(r_max_vals)-window):\n",
    "                        diffs_r_max[window].append(abs(r_max_vals[window+ind_start] - r_max_vals[ind_start])) \n",
    "\n",
    "    return diffs_r_max \n",
    "\n",
    "\n",
    "def calculateFluctuationValues(diffs_r_max):\n",
    "    ''' Calculates L(t) = <abs(Rmax(t+tau) - Rmax(t))> for tau = 0:numberOfFrames\n",
    "    \n",
    "    Input:\n",
    "        diffs_r_max = dictionary of r_max diffs, output of `getRmaxDiffs`\n",
    "        \n",
    "    Output:\n",
    "        dictionary, where key=tau, value=<abs(Rmax(t+tau) - Rmax(t))>\n",
    "    '''\n",
    "    \n",
    "    return {window: mean(diffs) for window, diffs in diffs_r_max.iteritems()}\n",
    "\n",
    "def plotFluctuationValues(fluct_values, plateau_range=None):\n",
    "    ''' Plots L(t) vs t (in frames)\n",
    "    \n",
    "    Input:\n",
    "        fluct_values = output from `calculateFluctuationValues`\n",
    "        plateau_range (optional) = if provided, calculates plateau by taking average\n",
    "                                    over frame range provided [start, end]\n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "    plt.plot(fluct_values.keys(), fluct_values.values(), '-o')\n",
    "    plt.xlabel('frame', fontsize=15);\n",
    "    plt.ylabel('<abs(Rmax(t+tau) - Rmax(t))> (pixels)', fontsize=15);\n",
    "    \n",
    "    #fluctvals=fluct_values.values()\n",
    "    #return fluctvals\n",
    "    \n",
    "    if plateau_range:\n",
    "        plateau = mean(fluct_values.values()[plateau_range[0]:plateau_range[1]])\n",
    "        print 'Plateau = %.6f pixels' % plateau\n",
    "        plt.plot([fluct_values.keys()[0], fluct_values.keys()[-1]], [plateau, plateau], 'k-');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calculate Rmax and Rmin values\n",
    "Calculates R_max and R_min for each frame using the centered videos created in Particle Tracking step. Results in \"all_r_max_min\", a dictionary containing the length of R_max and R_min for each frame along with the points for drawing both axes. \n",
    "\n",
    "Can also plot the lines of major/minor axis on each frame but takes significantly longer. Leave \"plot=False\" for **no plotting** or leave \"plot=True\" to **dump images** with axes overlayed for every frame to disk. Useful for confirming results (and troubleshooting weird results). In the past, I've set \"plot=True\" but then interrupt the kernel after maybe 45s of plotting -- you'll still get a good collection of images to look at and trouble check. Stored in 'r_max_min' folder in parent data folder for the day/condition being analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conformation = \"Ring\"\n",
    "#Condition = \"Actin\"\n",
    "#Date = \"041519\"\n",
    "\n",
    "#f_root = \"\\\\\\ANDERSONLAB\\\\AndersonLab\\\\Kathryn_R\\\\DATA\\\\DNA_in_XL_systems\\\\\n",
    "#Lightsheet\\\\\"+Conformation+\"\\\\\"+Condition+\"\\\\Data\\\\\"\n",
    "#subdir_names=[Conformation+Condition+\"_\"+group+\"of10_1000\"]\n",
    "datasets = loadMultipleDatasets(f_root, subdir_names)\n",
    "calculateConformationsForDatasets(datasets, f_root, subdir_names, plot=False)\n",
    "\n",
    "\n",
    "# load results dict\n",
    "all_r_max_min = loadMultipleConformationalDatasets(f_root, subdir_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Want to save your R_max, R_min values? Hint: you probably do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Date=\"050919\"\n",
    "#mass_used=\"2000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#f_root = \"\\\\\\ANDERSONLAB\\\\AndersonLab\\\\Kathryn_R\\\\DATA\\\\DNA_in_XL_systems\\\\Lightsheet\\\\\"+Conformation+\"\\\\\"+Condition+\"\\\\Data\\\\\"\n",
    "\n",
    "rmax=[]\n",
    "rmin=[]\n",
    "\n",
    "\n",
    "for subdir_name in subdir_names:\n",
    "    centered_frames_loc = os.path.join(f_root, subdir_name, 'centered_frames')\n",
    "    videos =  [video for video in os.listdir(centered_frames_loc) if video.endswith('.p')]\n",
    "    num1=subdir_name\n",
    "    for video in videos:\n",
    "        num2 = video\n",
    "        r=len(all_r_max_min[num1][num2].keys())\n",
    "        particle=all_r_max_min[num1][num2].keys()\n",
    "        #print r\n",
    "        #print particle\n",
    "        for listing in range(0,r):\n",
    "            num3=particle[listing]\n",
    "            tracked_length=(all_r_max_min[num1][num2][num3].keys())\n",
    "            length2=len(tracked_length)\n",
    "            for num4 in range(0,length2):\n",
    "                r_max=all_r_max_min[num1][num2][num3][num4].values()[1]\n",
    "                rmax.append(r_max)\n",
    "                r_min=all_r_max_min[num1][num2][num3][num4].values()[2]\n",
    "                rmin.append(r_min)\n",
    "\n",
    "\n",
    "\n",
    "w=open(f_root+Conformation+Condition+\"_\"+group+\"_\"+Date+\"\\\\\"+Conformation+Condition+\"_\"+group+\"_\"+Date+\"_r_max_values.csv\",'wb')\n",
    "writeFile=csv.writer(w)\n",
    "for n in rmax:\n",
    "    writeFile.writerow([n])\n",
    "w.close()\n",
    "\n",
    "t=open(f_root+Conformation+Condition+\"_\"+group+\"_\"+Date+\"\\\\\"+Conformation+Condition+\"_\"+group+\"_\"+Date+\"_r_min_values.csv\",\"wb\")\n",
    "writeFile=csv.writer(t)\n",
    "for m in rmin: \n",
    "    writeFile.writerow([m])\n",
    "t.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculating Fluctuation Values L(t)\n",
    "\n",
    "Calculates L(t) = <|Rmax(t+tau) - Rmax(t)|> from all_r_max_min dictionary, calculated at start of conformational analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plateau_range = [25,35]\n",
    "\n",
    "diffs_r_max = getRmaxDiffs(all_r_max_min)\n",
    "fluct_values = calculateFluctuationValues(diffs_r_max)\n",
    "plotFluctuationValues(fluct_values, plateau_range=plateau_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fluctvals=fluct_values.values()\n",
    "frames=fluct_values.keys()\n",
    "\n",
    "\n",
    "w=open(f_root+Conformation+Condition+\"_\"+group+\"_\"+Date+\"\\\\\"+Conformation+Condition+\"_\"+group+\"_\"+Date+\"_Fluct_Vals.csv\",'wb')\n",
    "writeFile=csv.writer(w)\n",
    "\n",
    "for n in fluctvals:\n",
    "    writeFile.writerow([n])\n",
    "    \n",
    "w.close()\n",
    "\n",
    "#'''\n",
    "#f=open(f_root+\"Data\\\\\\\\frames.csv\",'wb')\n",
    "#writeFile=csv.writer(f)\n",
    "\n",
    "#for n in frames:\n",
    " #   writeFile.writerow([n])\n",
    "    \n",
    "#w.close()\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dot Product Measurements\n",
    "**USES RMAX, RMIN VALUES YOU JUST RAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateDotProduct(delta_1, delta_2):\n",
    "    ''' Calculates dot product between two vectors with base (0, 0)\n",
    "    \n",
    "    Input:\n",
    "        delta_1 = (x_1, y_1)\n",
    "        delta_2 = (x_2, y_2)\n",
    "        \n",
    "    Output:\n",
    "        delta_1 __dot__ delta_2\n",
    "    '''\n",
    "    total = 0\n",
    "    for i, j in zip(delta_1, delta_2):\n",
    "        total += i*j\n",
    "        \n",
    "    return total\n",
    "\n",
    "def calculateAngleFromDeltas(delta):\n",
    "    ''' calculates angle from vector with base (0, 0)\n",
    "    \n",
    "    Input:\n",
    "        delta = (x, y)\n",
    "        \n",
    "    Output:\n",
    "        arctan(y/x)\n",
    "    '''\n",
    "    return math.atan(delt[1]/float(delt[0]))\n",
    "\n",
    "def calculateDeltas(data, major_or_minor):\n",
    "    ''' normalize 2 points into single point with base (0, 0)\n",
    "    and length 1\n",
    "    \n",
    "    Input:\n",
    "        delta = ((x_1, y_1), (x_2, y_2))\n",
    "        \n",
    "    Output:\n",
    "        (x_new, y_new)\n",
    "    '''\n",
    "    dmy = data[major_or_minor]\n",
    "    delt_x = dmy[0][0] - dmy[1][0]\n",
    "    delt_y = dmy[0][1] - dmy[1][1]\n",
    "    norm_factor = (delt_x**2 + delt_y**2)**0.5\n",
    "    \n",
    "    return (delt_x/norm_factor, delt_y/norm_factor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDotProducts(all_r_max_min, major_or_minor):\n",
    "    ''' Calculates dot products for all potential time windows. Similar\n",
    "    to `getRmaxDiffs` method. \n",
    "    \n",
    "    Input:\n",
    "        all_r_max_min = dictionary of conformational datasets, output of `loadMultipleConformationalDatasets`\n",
    "        major_or_minor = 'major' or 'minor' axis\n",
    "        \n",
    "    Output:\n",
    "        dictionary of dot products for each window size        \n",
    "    '''\n",
    "    if major_or_minor not in ('major', 'minor'):\n",
    "        raise Exception(\"Only 'major' or 'minor' allowed for `major_or_minor` arg. Received %s\")\n",
    "    \n",
    "    dot_prods = defaultdict(list)\n",
    "    \n",
    "    # iterate through datasets\n",
    "    for dataset, data in all_r_max_min.iteritems():\n",
    "        # iterate through videos\n",
    "        for video, particles in data.items():\n",
    "            # iterate through particles\n",
    "            for particle, conform_data in particles.items():\n",
    "                axes_data = [calculateDeltas(data, major_or_minor) for data in conform_data.values()]\n",
    "                \n",
    "                # iterate through possible sliding window sizes\n",
    "                for window in range(0, len(axes_data)):\n",
    "                    for ind_start in range(len(axes_data)-window):\n",
    "                        dot_prod = calculateDotProduct( \\\n",
    "                                axes_data[window+ind_start], axes_data[ind_start])\n",
    "                        \n",
    "                        if np.isnan(dot_prod):\n",
    "                            continue\n",
    "                        \n",
    "                        dot_prods[window].append(dot_prod)\n",
    "                             \n",
    "    return dot_prods\n",
    "\n",
    "def meanDotProds(dot_prods):\n",
    "    ''' Calculates mean dot product for each frame\n",
    "    \n",
    "    Input:\n",
    "        dot_prods = output from `getDotProducts`\n",
    "        \n",
    "    Output:\n",
    "        dict of mean dot products {frame: mean_dot_product}\n",
    "        \n",
    "    '''\n",
    "    return {frame: mean(prods) for frame, prods in dot_prods.iteritems()}\n",
    "\n",
    "def plotDotProds(dot_prods, major_or_minor):\n",
    "    ''' Plots dot products data\n",
    "    \n",
    "    Input:\n",
    "        dot_prods = output from `meanDotProds`\n",
    "        major_or_minor = 'major' or 'minor' axis\n",
    "        \n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    plt.figure();\n",
    "    plt.plot(dot_prods.keys(), dot_prods.values(), '-o');\n",
    "    plt.xlabel('frame', fontsize=20);\n",
    "    plt.title('%s axis' % major_or_minor, fontsize=20);\n",
    "    plt.ylabel('<%s(t+tau) __dot__ %s(t)>' % (major_or_minor, major_or_minor), fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating, Saving Dot products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "major_or_minor = 'major'\n",
    "dot_prods = getDotProducts(all_r_max_min, major_or_minor)\n",
    "mean_dot_prods = meanDotProds(dot_prods)\n",
    "plotDotProds(mean_dot_prods, major_or_minor)\n",
    "\n",
    "mean_dotprods=mean_dot_prods.values()\n",
    "time=mean_dot_prods.keys()\n",
    "\n",
    "#Date= \"050919\"\n",
    "w=open(f_root+Conformation+Condition+\"_\"+group+\"_\"+Date+\"\\\\\"+Conformation+Condition+\"_\"+group+\"_\"+Date+\"_mean_dot_prod_major.csv\",'wb')\n",
    "writeFile=csv.writer(w)\n",
    "\n",
    "for n in mean_dotprods:\n",
    "    writeFile.writerow([n])\n",
    "    \n",
    "w.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "major_or_minor = 'minor'\n",
    "dot_prods = getDotProducts(all_r_max_min, major_or_minor)\n",
    "mean_dot_prods = meanDotProds(dot_prods)\n",
    "plotDotProds(mean_dot_prods, major_or_minor)\n",
    "\n",
    "mean_dotprods=mean_dot_prods.values()\n",
    "time=mean_dot_prods.keys()\n",
    "\n",
    "#Date= \"042419\"\n",
    "w=open(f_root+Conformation+Condition+\"_\"+group+\"_\"+Date+\"\\\\\"+Conformation+Condition+\"_\"+group+\"_\"+Date+\"_mean_dot_prod_minor.csv\",'wb')\n",
    "writeFile=csv.writer(w)\n",
    "\n",
    "for n in mean_dotprods:\n",
    "    writeFile.writerow([n])\n",
    "    \n",
    "w.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ergodicity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateMsdByParticle(datasets):\n",
    "    ''' Calculates MSD for each individual particle, rather than combining across\n",
    "    all particles and then calculating MSD (like in `calculateMSDs` method)\n",
    "    \n",
    "    Input:\n",
    "        datasets = output from `loadMultipleDatasets`\n",
    "    \n",
    "    Output:\n",
    "        d_2_dict = dict of individual MSDs\n",
    "    '''\n",
    "    d_2_dict = {'x': [],\n",
    "                'y': [],\n",
    "                'comb': [],}\n",
    "\n",
    "    # iterate through datasets\n",
    "    for dataset, data in datasets.iteritems():\n",
    "        # iterate through videos\n",
    "        for video, particles in data['all_centers_dict_normed'].items():\n",
    "            # iterate through particles\n",
    "            for particle, center_data in particles.items():\n",
    "                x_centers = center_data['x']\n",
    "                y_centers = center_data['y']\n",
    "\n",
    "                x_msd = {}\n",
    "                y_msd = {}\n",
    "                c_msd = {}\n",
    "                # iterate through possible sliding window sizes\n",
    "                for window in range(1, len(x_centers)):\n",
    "                    x_msd[window] = []\n",
    "                    y_msd[window] = []\n",
    "                    c_msd[window] = []\n",
    "                    for ind_start in range(len(x_centers)-window):\n",
    "                        x_msd[window].append(x_centers[window+ind_start] - x_centers[ind_start]) \n",
    "                        y_msd[window].append(y_centers[window+ind_start] - y_centers[ind_start])\n",
    "                        c_msd[window].append(x_msd[window][-1])\n",
    "                        c_msd[window].append(y_msd[window][-1])\n",
    "\n",
    "                dmy_dict = {'x': x_msd,\n",
    "                           'y': y_msd,\n",
    "                           'comb': c_msd}\n",
    "\n",
    "                # calculate MSD for individual particle\n",
    "                dmy_dict_msd = calculateMSDs(dmy_dict)\n",
    "                d_2_dict['x'].append(dmy_dict_msd['x'])\n",
    "                d_2_dict['y'].append(dmy_dict_msd['y'])\n",
    "                d_2_dict['comb'].append(dmy_dict_msd['comb'])\n",
    "\n",
    "    return d_2_dict\n",
    "\n",
    "def squareIndividualMsds(d_2_dict):\n",
    "    ''' Square each of the individual MSDs\n",
    "    \n",
    "    Input:\n",
    "        d_2_dict = output from `calculateMsdByParticle`\n",
    "    \n",
    "    Output:\n",
    "        d_2_dict_2 = dict of individual MSDs squared\n",
    "    '''\n",
    "    d_2_dict_2 = {'x': [],\n",
    "                  'y': [],\n",
    "                  'comb': [],}\n",
    "\n",
    "    for axis, particles in d_2_dict.items():\n",
    "        for particle in particles:\n",
    "            new_dict = {}\n",
    "            for frame, px_2 in particle.items():\n",
    "                new_dict[frame] = px_2**2\n",
    "\n",
    "            d_2_dict_2[axis].append(new_dict)\n",
    "            \n",
    "    return d_2_dict_2\n",
    "\n",
    "def averageAcrossMsds(d_2_dict_2):\n",
    "    ''' Average across each of the individual MSDs, tracking\n",
    "    how many particles exist for each frame number (ie, every one of\n",
    "    the N particles will have a frame 1 and need to be normalized \n",
    "    by N, but not every particle will extend out to frame f_max. If\n",
    "    only 1 particle is f_max frames long, then the average is simply the\n",
    "    single value at f_max/1, NOT divided by N).\n",
    "    \n",
    "    Input:\n",
    "        d_2_dict_2 = output from `squareIndividualMsds`\n",
    "    \n",
    "    Output:\n",
    "        d_2_dict_2_average = dict of averaged values across d_2_dict_2\n",
    "    '''\n",
    "    def _emptyList():\n",
    "        return [0, 0]\n",
    "\n",
    "    avg_across_particles = {'x': defaultdict(_emptyList),\n",
    "                            'y': defaultdict(_emptyList),\n",
    "                            'comb': defaultdict(_emptyList),}\n",
    "\n",
    "    for axis, particles in d_2_dict_2.items():\n",
    "        for particle in particles:\n",
    "            for frame, px_4 in particle.items():\n",
    "                avg_across_particles[axis][frame][0] += 1\n",
    "                avg_across_particles[axis][frame][1] += px_4\n",
    "\n",
    "    d_2_dict_2_average = {}\n",
    "    for axis, data in avg_across_particles.iteritems():\n",
    "        d_2_dict_2_average[axis] = {}\n",
    "        for frame, frame_data in data.items():\n",
    "            d_2_dict_2_average[axis][frame] = frame_data[1]/float(frame_data[0])\n",
    "\n",
    "    return d_2_dict_2_average\n",
    "\n",
    "\n",
    "def squareStandardMsds(msd_dict):\n",
    "    ''' Squares standard MSD values to get second term required\n",
    "    to calculate EB\n",
    "    \n",
    "    Input:\n",
    "        msd_dict = output from `calculateMSDs`\n",
    "    \n",
    "    Output:\n",
    "        msd_dict_2 = msd_dict with each value squared\n",
    "    '''\n",
    "    msd_dict_2 = {'x': [],\n",
    "                  'y': [],\n",
    "                  'comb': [],}\n",
    "\n",
    "    for axis, frame_data in msd_dict.items():\n",
    "        new_dict = {}\n",
    "        for frame, px_2 in frame_data.items():\n",
    "            new_dict[frame] = px_2**2\n",
    "\n",
    "        msd_dict_2[axis] = new_dict\n",
    "        \n",
    "    return msd_dict_2\n",
    "\n",
    "def calculateEB(d_2_dict_2_average, msd_dict_2):\n",
    "    ''' Calculates EB from the values required for the numerator +\n",
    "    denominator by frame as defined in equation 15 from:\n",
    "    \n",
    "    Nonergodicity, fluctuations, and criticality in heterogeneous diffusion processes\n",
    "    PHYSICAL REVIEW E 90, 012134 (2014)\n",
    "    \n",
    "    Input:\n",
    "        d_2_dict_2_average = output from `averageAcrossMsds`\n",
    "        msd_dict_2 = output from `squareStandardMsds`\n",
    "    \n",
    "    Output:\n",
    "        eb = eb by frame (for each axis)\n",
    "    '''\n",
    "    eb = {}\n",
    "    for axis in ('x', 'y', 'comb'):\n",
    "        eb[axis] = {}\n",
    "        for frame, val in d_2_dict_2_average[axis].iteritems():\n",
    "            if frame == 0:\n",
    "                continue \n",
    "\n",
    "            term_1 = val\n",
    "            term_2 = msd_dict_2[axis][frame]\n",
    "            eb[axis][frame] = (term_1 - term_2)/float(term_2)\n",
    "\n",
    "    return eb\n",
    "\n",
    "def calculateEBFromDatasets(datasets):\n",
    "    ''' Runs through entire process to calculate and return EB(frame)\n",
    "    from just datasets.\n",
    "    \n",
    "    Input:\n",
    "        datasets = output from `loadMultipleDatasets`\n",
    "        \n",
    "    Output:\n",
    "        eb = eb by frame (for each axis)\n",
    "    '''\n",
    "    d_2_dict = calculateMsdByParticle(datasets)\n",
    "    d_2_dict_2 = squareIndividualMsds(d_2_dict)\n",
    "    d_2_dict_2_average = averageAcrossMsds(d_2_dict_2)\n",
    "\n",
    "    diffs_dict = getDiffs(datasets)\n",
    "    msd_dict = calculateMSDs(diffs_dict)\n",
    "    msd_dict_2 = squareStandardMsds(msd_dict)\n",
    "    \n",
    "    return calculateEB(d_2_dict_2_average, msd_dict_2)\n",
    "\n",
    "def calculateGParameterFromDatasets(datasets, d=1):\n",
    "    ''' Runs through entire process to calculate and return G(frame)\n",
    "    from just datasets as defined by equation 19 from:\n",
    "    \n",
    "    Nonergodicity, fluctuations, and criticality in heterogeneous diffusion processes\n",
    "    PHYSICAL REVIEW E 90, 012134 (2014)\n",
    "    \n",
    "    Input:\n",
    "        datasets = output from `loadMultipleDatasets`\n",
    "        \n",
    "    Output:\n",
    "        G = G by frame (for each axis)\n",
    "    '''\n",
    "    diffs_dict = getDiffs(datasets)\n",
    "    msd_dict = calculateMSDs(diffs_dict)\n",
    "    denom_dict = squareStandardMsds(msd_dict)\n",
    "\n",
    "    squared_diffs_dict = {}\n",
    "    for axis, axis_dict in diffs_dict.iteritems():\n",
    "        squared_diffs_dict[axis] = {}\n",
    "        for window, diffs in axis_dict.iteritems():\n",
    "            squared_diffs_dict[axis][window] = [diff**2 for diff in diffs]\n",
    "\n",
    "    numer_dict = calculateMSDs(squared_diffs_dict)           \n",
    "\n",
    "    G = {}\n",
    "    for axis, numer_data in numer_dict.iteritems():\n",
    "        denom_data = denom_dict[axis]\n",
    "        G[axis] = {frame: d/(d+2.) * numer_data[frame]/float(denom_data[frame]) - 1 \\\n",
    "               for frame in numer_data.keys() if frame > 0}\n",
    "\n",
    "    return G\n",
    "\n",
    "def plotSingleAxis(single_data_dict, axis, y_label):\n",
    "    ''' Takes a dictionary of data, designed for both\n",
    "    eb, G and plots a single axis.\n",
    "    \n",
    "    Input:\n",
    "        single_data_dict =  dict of data to be plotted\n",
    "        axis = string to clarify which axis is being plotted\n",
    "        y_label = string to label the y-axis\n",
    "        \n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "    plt.figure();\n",
    "    plt.plot(single_data_dict.keys(), single_data_dict.values(), '-o');\n",
    "    plt.xlabel('frame', fontsize=20)\n",
    "    plt.ylabel(y_label, fontsize=20)\n",
    "    plt.title('Axis = %s' % axis, fontsize=20)\n",
    "    \n",
    "def plotAllAxes(data_dict, y_label):\n",
    "    ''' Takes a dictionary of data, designed for both\n",
    "    eb, G and plots all axes.\n",
    "    \n",
    "    Input:\n",
    "        data_dict =  dict of data to be plotted (designed for G or eb)\n",
    "        y_label = string to label the y-axis\n",
    "        \n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "    for axis, single_data_dict in data_dict.iteritems():\n",
    "        plotSingleAxis(single_data_dict, axis, y_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = loadMultipleDatasets(f_root, subdir_names)\n",
    "eb = calculateEBFromDatasets(datasets)\n",
    "#plotAllAxes(eb, 'EB(frame)')\n",
    "#plt.show()\n",
    "\n",
    "G = calculateGParameterFromDatasets(datasets)\n",
    "#plotAllAxes(G, 'G(frame)')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#G = calculateGParameterFromDatasets(datasets)\n",
    "plotAllAxes(G, 'G(frame)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conformation = \"Ring\"\n",
    "#Condition = \"CoXL\"\n",
    "axis = \"comb\"\n",
    "EB=[]\n",
    "#Date=\"052119\"\n",
    "for v in range(0,390):\n",
    "    EBValues_List=eb[axis].values()\n",
    "    Add_EBVal=EBValues_List[v]\n",
    "    EB.append(Add_EBVal)\n",
    "    \n",
    "w=open(f_root+Conformation+\"_\"+mass_used+\"\\\\\"+Conformation+date+\"_EBvalues.csv\",\"wb\")\n",
    "writeFile=csv.writer(w)\n",
    "\n",
    "for n in EB:\n",
    "    writeFile.writerow([n])\n",
    "    \n",
    "w.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "axis = \"comb\"\n",
    "Gvals=[]\n",
    "\n",
    "for v in range(0,390):\n",
    "    Gvalues_List=G[axis].values()\n",
    "    Add_Gval=Gvalues_List[v]\n",
    "    Gvals.append(Add_Gval)\n",
    "    \n",
    "w=open(f_root+Conformation+\"_\"+mass_used+\"\\\\\"+Conformation+date+\"_Gvalues.csv\",\"wb\")\n",
    "writeFile=csv.writer(w)\n",
    "\n",
    "for n in Gvals:\n",
    "    writeFile.writerow([n])\n",
    "    \n",
    "w.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=calculateMsdByParticle(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=open(f_root+Conformation+\"_\"+mass_used+\"\\\\\"+Conformation+date+\"_SingleTracks.csv\", \"wb\")\n",
    "fieldnames = range(1999)\n",
    "writeFile=csv.DictWriter(w,fieldnames,extrasaction='ignore')\n",
    "for i in range(len(test['comb'])):\n",
    "    writeFile.writerow(test['comb'][i])\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
